{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f3754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing test set images\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test/'\n",
    "\n",
    "#os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "total_width = 0\n",
    "total_height = 0\n",
    "num_images = 0\n",
    "\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            total_width += width\n",
    "            total_height += height\n",
    "            num_images += 1\n",
    "            \n",
    "average_width = total_width / num_images\n",
    "average_height = total_height / num_images\n",
    "\n",
    "print(\"Average width:\", average_width)\n",
    "print(\"Average height:\", average_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d67c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing test set images\n",
    "average_width = int(average_width)\n",
    "average_height = int(average_height)\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test/'\n",
    "output_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test1/'\n",
    "target_size = (average_width, average_height)  # Specify the desired target size (width, height)\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode in (\"RGBA\", \"P\"): \n",
    "                image = image.convert(\"RGB\")\n",
    "            resized_image = image.resize(target_size)\n",
    "\n",
    "            output_filename = os.path.join(output_path+folder, filename)\n",
    "            os.makedirs(output_path+folder, exist_ok=True)\n",
    "            resized_image.save(output_filename)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Resizing completed for the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6629ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/train/'\n",
    "\n",
    "#os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "total_width = 0\n",
    "total_height = 0\n",
    "num_images = 0\n",
    "\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            total_width += width\n",
    "            total_height += height\n",
    "            num_images += 1\n",
    "            \n",
    "average_width = total_width / num_images\n",
    "average_height = total_height / num_images\n",
    "\n",
    "print(\"Average width:\", average_width)\n",
    "print(\"Average height:\", average_height)\n",
    "average_width = int(average_width)\n",
    "average_height = int(average_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c4759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing train set images\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/train/'\n",
    "output_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/train1/'\n",
    "target_size = (average_width, average_height)  \n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode in (\"RGBA\", \"P\"): \n",
    "                image = image.convert(\"RGB\")\n",
    "            resized_image = image.resize(target_size)\n",
    "\n",
    "            output_filename = os.path.join(output_path+folder, filename)\n",
    "            os.makedirs(output_path+folder, exist_ok=True)\n",
    "            resized_image.save(output_filename)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Resizing completed for the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f24d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/val/'\n",
    "\n",
    "#os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "total_width = 0\n",
    "total_height = 0\n",
    "num_images = 0\n",
    "\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            width, height = image.size\n",
    "            total_width += width\n",
    "            total_height += height\n",
    "            num_images += 1\n",
    "            \n",
    "average_width = total_width / num_images\n",
    "average_height = total_height / num_images\n",
    "\n",
    "print(\"Average width:\", average_width)\n",
    "print(\"Average height:\", average_height)\n",
    "average_width = int(average_width)\n",
    "average_height = int(average_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a02450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resizing val set images\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/val/'\n",
    "output_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/val1/'\n",
    "target_size = (average_width,average_height)  # Specify the desired target size (width, height)\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    print(dataset_path+folder)\n",
    "    for filename in os.listdir(dataset_path+folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "            image_path = os.path.join(dataset_path+folder, filename)\n",
    "            image = Image.open(image_path)\n",
    "            if image.mode in (\"RGBA\", \"P\"): \n",
    "                image = image.convert(\"RGB\")\n",
    "            resized_image = image.resize(target_size)\n",
    "\n",
    "            output_filename = os.path.join(output_path+folder, filename)\n",
    "            os.makedirs(output_path+folder, exist_ok=True)\n",
    "            resized_image.save(output_filename)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Resizing completed for the dataset.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec1c1e26",
   "metadata": {},
   "source": [
    "#resizing for individual folder \n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dataset_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test/'\n",
    "output_path = r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test1/'\n",
    "target_size = (256, 256)  # Specify the desired target size (width, height)\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "folder = '56'\n",
    "print(dataset_path+folder)\n",
    "for filename in os.listdir(dataset_path+folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):  \n",
    "        image_path = os.path.join(dataset_path+folder, filename)\n",
    "        image = Image.open(image_path)\n",
    "        if image.mode in (\"RGBA\", \"P\"): \n",
    "            image = image.convert(\"RGB\")\n",
    "        resized_image = image.resize(target_size)\n",
    "\n",
    "        output_filename = os.path.join(output_path+folder, filename)\n",
    "        os.makedirs(output_path+folder, exist_ok=True)\n",
    "        resized_image.save(output_filename)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Resizing completed for the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train=r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/train.txt'\n",
    "df_train=pd.read_csv(path_train,sep='\\s+',names=['name','label'],index_col=False)\n",
    "data_dict_train = {row['name']: row['label'] for _, row in df_train.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in glob.glob(r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/train1/0/*'):\n",
    "    image=cv2.imread(i)\n",
    "    p_image=tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    X_train.append(p_image)\n",
    "    y_train.append(data_dict_train[Path(i).stem+'.jpg'])\n",
    "print(p_image.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_val=r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/val.txt'\n",
    "df_val=pd.read_csv(path_val,sep='\\s+',names=['name','label'],index_col=False)\n",
    "data_dict_val = {row['name']: row['label'] for _, row in df_val.iterrows()}\n",
    "print(data_dict_val['03948.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf70e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val=[]\n",
    "y_val=[]\n",
    "for i in glob.glob(r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/val1/0/*'):\n",
    "    image=cv2.imread(i)\n",
    "    p_image=tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    X_val.append(p_image)\n",
    "    y_val.append(data_dict_val[Path(i).stem+'.jpg'])\n",
    "print(p_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test=r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/test.txt'\n",
    "df_test=pd.read_csv(path_test,sep='\\s+',names=['name','label'],index_col=False)\n",
    "data_dict_test = {row['name']: row['label'] for _, row in df_test.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "y_test=[]\n",
    "for i in glob.glob(r'C:/Users/nikhi/OneDrive/Desktop/Sem4/data/classification/test1/0/*'):\n",
    "    image=cv2.imread(i)\n",
    "    p_image=tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    X_test.append(p_image)\n",
    "    y_test.append(data_dict_test[Path(i).stem+'.jpg'])\n",
    "print(p_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6013e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val) \n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d0384ef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(type(X_val))\n",
    "print(type(y_val))\n",
    "\n",
    "num_classes = 2\n",
    "# Load the EfficientNetB0 model without the top (classification) layers\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the weights of the pre-trained layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the input shape as None, which allows for variable input sizes\n",
    "input_shape = (None, None, 3)\n",
    "\n",
    "# Create a global average pooling layer to adapt to variable input sizes\n",
    "global_average_layer = GlobalAveragePooling2D()\n",
    "\n",
    "# Add your custom classification head\n",
    "classification_head = Dense(num_classes, activation='softmax')\n",
    "\n",
    "# Build the model by chaining the layers together\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "x = base_model(inputs, training=False)\n",
    "x = global_average_layer(x)\n",
    "outputs = classification_head(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_encoded = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_val, y_val_encoded))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e52a3fb",
   "metadata": {},
   "source": [
    "print(len(X_val),len(y_val))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d943465c",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {loss:.4f}\") \n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea4a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
